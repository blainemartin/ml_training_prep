{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Introduction"
      },
      "source": [
        "# Launching with NVIDIA Llama 3.1 Nemotron 70B Instruct HF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ModelSelection",
        "cellView": "form"
      },
      "source": [
        "#@title Select the Huggingface model\n",
        "model_name = \"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InstallLibraries"
      },
      "source": [
        "## Install Required Libraries\n",
        "Please note, a session restart might be required before starting the chat interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InstallDependencies"
      },
      "source": [
        "!pip install transformers accelerate bitsandbytes gradio --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImportLibraries"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImportCode"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import gradio as gr\n",
        "from accelerate import init_empty_weights, infer_auto_device_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoadModel"
      },
      "source": [
        "## Load the Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoadTokenizer"
      },
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CheckGPUs"
      },
      "source": [
        "# Check available GPUs\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Number of GPUs available: {num_gpus}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoadModelCode"
      },
      "source": [
        "# Define the max memory per GPU\n",
        "max_memory = {i: '80GB' for i in range(num_gpus)}\n",
        "\n",
        "# Load the model using device_map 'auto'\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    max_memory=max_memory,\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DefineChatFunction"
      },
      "source": [
        "## Define the Chat Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChatFunction"
      },
      "source": [
        "def chat(input_text, history=[]):\n",
        "    history = history or []\n",
        "    history.append({\"role\": \"user\", \"content\": input_text})\n",
        "\n",
        "    # Build the conversation prompt\n",
        "    prompt = \"\"\n",
        "    for turn in history:\n",
        "        role = turn[\"role\"].capitalize()\n",
        "        content = turn[\"content\"]\n",
        "        prompt += f\"{role}: {content}\\n\"\n",
        "    prompt += \"Assistant:\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=16000,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=True\n",
        "    )\n",
        "    response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "    response = response.strip()\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "    return \"\", history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CreateInterface"
      },
      "source": [
        "## Create the Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GradioInterface"
      },
      "source": [
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        return chat(message, chat_history)\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SampleUsage"
      },
      "source": [
        "## Sample Usage\n",
        "You can now interact with the model using the chat interface above.\n",
        "\n",
        "### Example Prompts:\n",
        "- \"Can you list five recommended tourist destinations in Japan?\"\n",
        "- \"What is the second highest mountain in Japan?\"\n",
        "- \"Summarize the following text...\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TestPrompt"
      },
      "source": [
        "# Example of generating a response programmatically\n",
        "input_text = \"Can you list five recommended tourist destinations in Japan?\"\n",
        "_, history = chat(input_text)\n",
        "print(history[-1][\"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "sha256:16c76f..."
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
